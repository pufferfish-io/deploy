name: ðŸš€ Deploy Kafka (KRaft, single node)

on:
  workflow_dispatch:

jobs:
  deploy:
    name: ðŸ”„ Deploy Kafka (KRaft) into K3s
    runs-on: self-hosted

    env:
      # K3s kubeconfig path used across existing workflows
      KUBECONFIG: /etc/rancher/k3s/k3s.yaml

    steps:
      - name: ðŸ§¾ Checkout repo
        uses: actions/checkout@v4

      - name: ðŸ”§ Ensure namespace
        run: |
          # Idempotent: create namespace only if it does not exist
          kubectl get ns app || kubectl create ns app

      - name: ðŸ“¦ Apply Kafka manifest (StatefulSet + Service)
        run: |
          set -euo pipefail

          # Create a temporary manifest with inline comments and resource limits
          cat > /tmp/kafka.yaml <<'YAML'
          # Namespace is applied to be selfâ€‘contained; safe if already exists
          apiVersion: v1
          kind: Namespace
          metadata:
            name: app
          ---
          # ClusterIP Service for clients (Kafka API 9092)
          apiVersion: v1
          kind: Service
          metadata:
            name: kafka
            namespace: app
            labels:
              app: kafka
          spec:
            type: ClusterIP
            selector:
              app: kafka
            ports:
              - name: kafka
                port: 9092
                targetPort: 9092
          ---
          # Headless Service for stable pod DNS
          apiVersion: v1
          kind: Service
          metadata:
            name: kafka-headless
            namespace: app
            labels:
              app: kafka
          spec:
            clusterIP: None
            publishNotReadyAddresses: true
            selector:
              app: kafka
            ports:
              - name: controller
                port: 9093
                targetPort: 9093
          ---
          # Singleâ€‘replica Kafka (KRaft) with light limits; inter-broker PLAINTEXT; client SASL/SCRAM512
          apiVersion: apps/v1
          kind: StatefulSet
          metadata:
            name: kafka
            namespace: app
            labels:
              app: kafka
          spec:
            serviceName: kafka
            replicas: 1
            selector:
              matchLabels:
                app: kafka
            template:
              metadata:
                labels:
                  app: kafka
              spec:
                terminationGracePeriodSeconds: 30
                securityContext:
                  fsGroup: 1001
                initContainers:
                  - name: format-storage
                    image: apache/kafka:3.8.0
                    command: ["/bin/sh","-lc"]
                    args:
                      - |
                        set -e
                        cd /opt/kafka
                        if [ ! -f /var/lib/kafka/data/meta.properties ]; then
                          CID=$(bin/kafka-storage.sh random-uuid)
                          echo "Formatting storage with CLUSTER_ID=$CID"
                          bin/kafka-storage.sh format -t "$CID" -c config/kraft/server.properties
                        else
                          echo "Storage already formatted"
                        fi
                    volumeMounts:
                      - name: data
                        mountPath: /var/lib/kafka/data
                containers:
                  - name: kafka
                    image: apache/kafka:3.8.0
                    imagePullPolicy: IfNotPresent
                    env:
                      # KRaft single-node
                      - name: KAFKA_CFG_PROCESS_ROLES
                        value: broker,controller
                      - name: KAFKA_CFG_NODE_ID
                        value: "1"
                      - name: KAFKA_CFG_CONTROLLER_QUORUM_VOTERS
                        value: "1@kafka-0.kafka-headless.app.svc.cluster.local:9093"
                      - name: KAFKA_CFG_CONTROLLER_LISTENER_NAMES
                        value: CONTROLLER

                      # Listeners: INTERNAL for clients (SASL_PLAINTEXT), ADMIN for local admin (PLAINTEXT), CONTROLLER for KRaft
                      - name: KAFKA_CFG_LISTENERS
                        value: INTERNAL://:9092,CONTROLLER://:9093,ADMIN://:9094
                      - name: KAFKA_CFG_ADVERTISED_LISTENERS
                        value: INTERNAL://kafka.app.svc.cluster.local:9092,ADMIN://kafka-0.kafka-headless.app.svc.cluster.local:9094
                      - name: KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP
                        value: INTERNAL:SASL_PLAINTEXT,CONTROLLER:PLAINTEXT,ADMIN:PLAINTEXT

                      # Use ADMIN (PLAINTEXT) for inter-broker
                      - name: KAFKA_CFG_INTER_BROKER_LISTENER_NAME
                        value: ADMIN

                      # Client SASL settings (for INTERNAL listener only)
                      - name: KAFKA_CFG_SASL_ENABLED_MECHANISMS
                        value: SCRAM-SHA-512
                      - name: KAFKA_CFG_LISTENER_NAME_INTERNAL_SASL_ENABLED_MECHANISMS
                        value: SCRAM-SHA-512

                      # Single node replication factors
                      - name: KAFKA_CFG_OFFSETS_TOPIC_REPLICATION_FACTOR
                        value: "1"
                      - name: KAFKA_CFG_TRANSACTION_STATE_LOG_REPLICATION_FACTOR
                        value: "1"
                      - name: KAFKA_CFG_TRANSACTION_STATE_LOG_MIN_ISR
                        value: "1"

                      # Data dir and light retention
                      - name: KAFKA_CFG_LOG_DIRS
                        value: "/var/lib/kafka/data"
                      - name: KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE
                        value: "true"
                      - name: KAFKA_CFG_NUM_PARTITIONS
                        value: "1"
                      - name: KAFKA_CFG_LOG_RETENTION_HOURS
                        value: "24"
                      - name: KAFKA_CFG_LOG_RETENTION_BYTES
                        value: "268435456"

                      # JVM heap small
                      - name: KAFKA_HEAP_OPTS
                        value: "-Xmx256m -Xms256m"
                    ports:
                      - containerPort: 9092
                        name: kafka
                      - containerPort: 9093
                        name: controller
                      - containerPort: 9094
                        name: admin
                    volumeMounts:
                      - name: data
                        mountPath: /var/lib/kafka/data
                    resources:
                      requests:
                        cpu: 200m
                        memory: 384Mi
                      limits:
                        cpu: 500m
                        memory: 700Mi
            volumeClaimTemplates:
              - metadata:
                  name: data
                spec:
                  accessModes: ["ReadWriteOnce"]
                  storageClassName: local-path
                  resources:
                    requests:
                      storage: 5Gi
          YAML

          # Apply manifest idempotently
          kubectl apply -f /tmp/kafka.yaml

          # Ensure the image tag is exactly what we want (avoid stale tag in template)
          kubectl -n app set image statefulset/kafka kafka=apache/kafka:3.8.0 --record

      - name: ðŸ”Ž Rollout status
        run: |
          set -e
          kubectl -n app rollout status statefulset/kafka --timeout=420s
          kubectl -n app get pods -l app=kafka -o wide

      - name: ðŸ” Ensure SASL user exists (SCRAM-SHA-512)
        env:
          SASL_USER: ${{ secrets.KAFKA_SASL_USERNAME }}
          SASL_PASS: ${{ secrets.KAFKA_SASL_PASSWORD }}
        run: |
          set -euo pipefail

          if [ -z "${SASL_USER:-}" ] || [ -z "${SASL_PASS:-}" ]; then
            echo "KAFKA_SASL_USERNAME / KAFKA_SASL_PASSWORD secrets are required" >&2
            exit 1
          fi

          # Wait for admin listener to accept connections, then create SCRAM user
          # If user exists, kafka-configs.sh --describe returns the SCRAM config; otherwise, we add it.
          kubectl -n app exec kafka-0 -- bash -lc '
            set -e
            # wait for admin listener readiness
            for i in $(seq 1 120); do
              if kafka-topics.sh --bootstrap-server 127.0.0.1:9094 --list >/dev/null 2>&1; then break; fi
              sleep 2
            done
            DESCRIBE=$(kafka-configs.sh --bootstrap-server 127.0.0.1:9094 \
              --describe --entity-type users --entity-name "'$SASL_USER'" || true)
            if echo "$DESCRIBE" | grep -q "Configs for user"; then
              echo "user exists: '$SASL_USER'"
            else
              kafka-configs.sh --bootstrap-server 127.0.0.1:9094 \
                --alter --add-config "SCRAM-SHA-512=[password='"'$SASL_PASS'"']" \
                --entity-type users --entity-name "'$SASL_USER'"
              echo "user created: '$SASL_USER'"
            fi'

      - name: ðŸ“£ Summary
        run: |
          echo "Bootstrap for services: kafka.app.svc.cluster.local:9092"
          echo "Ensure repo Variables are set for topic names and group/client IDs."
